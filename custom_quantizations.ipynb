{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFdPvlXBOdUN"
   },
   "source": [
    "# Quantization aware training comprehensive guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqnbd7TOfAq9"
   },
   "source": [
    "For finding the APIs you need and understanding purposes, you can run but skip reading this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "execution": {
     "iopub.execute_input": "2023-04-06T11:06:36.701652Z",
     "iopub.status.busy": "2023-04-06T11:06:36.701130Z",
     "iopub.status.idle": "2023-04-06T11:06:44.148832Z",
     "shell.execute_reply": "2023-04-06T11:06:44.147939Z"
    },
    "id": "lvpH1Hg7ULFz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 337ms/step - loss: 16.1181 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "import tempfile\n",
    "\n",
    "input_shape = [20]\n",
    "x_train = np.random.randn(1, 20).astype(np.float32)\n",
    "y_train = tf.keras.utils.to_categorical(np.random.randn(1), num_classes=20)\n",
    "\n",
    "def setup_model():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(20, input_shape=input_shape),\n",
    "      tf.keras.layers.Flatten()\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "def setup_pretrained_weights():\n",
    "  model= setup_model()\n",
    "\n",
    "  model.compile(\n",
    "      loss=tf.keras.losses.categorical_crossentropy,\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy']\n",
    "  )\n",
    "\n",
    "  model.fit(x_train, y_train)\n",
    "\n",
    "  _, pretrained_weights = tempfile.mkstemp('.tf')\n",
    "\n",
    "  model.save_weights(pretrained_weights)\n",
    "\n",
    "  return pretrained_weights\n",
    "\n",
    "def setup_pretrained_model():\n",
    "  model = setup_model()\n",
    "  pretrained_weights = setup_pretrained_weights()\n",
    "  model.load_weights(pretrained_weights)\n",
    "  return model\n",
    "\n",
    "setup_model()\n",
    "pretrained_weights = setup_pretrained_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTHLMLV-ZrUA"
   },
   "source": [
    "## Define quantization aware model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ybigft1fTn4T"
   },
   "source": [
    "### Quantize whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T11:06:44.153329Z",
     "iopub.status.busy": "2023-04-06T11:06:44.152641Z",
     "iopub.status.idle": "2023-04-06T11:06:45.503065Z",
     "shell.execute_reply": "2023-04-06T11:06:45.502358Z"
    },
    "id": "1s_EK8reOruu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLay  (None, 20)               3         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " quant_dense_2 (QuantizeWrap  (None, 20)               425       \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_flatten_2 (QuantizeWr  (None, 20)               1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 420\n",
      "Non-trainable params: 9\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = setup_model()\n",
    "base_model.load_weights(pretrained_weights) # optional but recommended for model accuracy\n",
    "\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(base_model)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTbTLn3dZM7h"
   },
   "source": [
    "### Quantize some layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T11:06:45.506562Z",
     "iopub.status.busy": "2023-04-06T11:06:45.506304Z",
     "iopub.status.idle": "2023-04-06T11:06:45.636875Z",
     "shell.execute_reply": "2023-04-06T11:06:45.636191Z"
    },
    "id": "HN0B_QB-ZhE2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_1 (QuantizeL  (None, 20)               3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_dense_3 (QuantizeWrap  (None, 20)               425       \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 20)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 428\n",
      "Trainable params: 420\n",
      "Non-trainable params: 8\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a base model\n",
    "base_model = setup_model()\n",
    "base_model.load_weights(pretrained_weights) # optional but recommended for model accuracy\n",
    "\n",
    "# Helper function uses `quantize_annotate_layer` to annotate that only the \n",
    "# Dense layers should be quantized.\n",
    "def apply_quantization_to_dense(layer):\n",
    "  if isinstance(layer, tf.keras.layers.Dense):\n",
    "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "  return layer\n",
    "\n",
    "# Use `tf.keras.models.clone_model` to apply `apply_quantization_to_dense` \n",
    "# to the layers of the model.\n",
    "annotated_model = tf.keras.models.clone_model(\n",
    "    base_model,\n",
    "    clone_function=apply_quantization_to_dense,\n",
    ")\n",
    "\n",
    "# Now that the Dense layers are annotated,\n",
    "# `quantize_apply` actually makes the model quantization aware.\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T11:06:45.640044Z",
     "iopub.status.busy": "2023-04-06T11:06:45.639705Z",
     "iopub.status.idle": "2023-04-06T11:06:45.643494Z",
     "shell.execute_reply": "2023-04-06T11:06:45.642847Z"
    },
    "id": "CjY_JyB808Da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_3\n"
     ]
    }
   ],
   "source": [
    "print(base_model.layers[0].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpb_BydRaSoF"
   },
   "source": [
    "#### More readable but potentially lower model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQoMH3g3fWwb"
   },
   "source": [
    "**Functional example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T11:06:45.646895Z",
     "iopub.status.busy": "2023-04-06T11:06:45.646466Z",
     "iopub.status.idle": "2023-04-06T11:06:45.769192Z",
     "shell.execute_reply": "2023-04-06T11:06:45.768495Z"
    },
    "id": "7Wow55hg5oiM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20)]              0         \n",
      "                                                                 \n",
      " quantize_layer_2 (QuantizeL  (None, 20)               3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_dense_4 (QuantizeWrap  (None, 10)               215       \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 218\n",
      "Trainable params: 210\n",
      "Non-trainable params: 8\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Use `quantize_annotate_layer` to annotate that the `Dense` layer\n",
    "# should be quantized.\n",
    "i = tf.keras.Input(shape=(20,))\n",
    "x = tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(10))(i)\n",
    "o = tf.keras.layers.Flatten()(x)\n",
    "annotated_model = tf.keras.Model(inputs=i, outputs=o)\n",
    "\n",
    "# Use `quantize_apply` to actually make the model quantization aware.\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "\n",
    "# For deployment purposes, the tool adds `QuantizeLayer` after `InputLayer` so that the\n",
    "# quantized model can take in float inputs instead of only uint8.\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIGj-r2of2ls"
   },
   "source": [
    "**Sequential example**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T11:06:45.772956Z",
     "iopub.status.busy": "2023-04-06T11:06:45.772388Z",
     "iopub.status.idle": "2023-04-06T11:06:45.882482Z",
     "shell.execute_reply": "2023-04-06T11:06:45.881783Z"
    },
    "id": "mQOiDUGgfi4y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_3 (QuantizeL  (None, 20)               3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_dense_5 (QuantizeWrap  (None, 20)               425       \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 20)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 428\n",
      "Trainable params: 420\n",
      "Non-trainable params: 8\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Use `quantize_annotate_layer` to annotate that the `Dense` layer\n",
    "# should be quantized.\n",
    "annotated_model = tf.keras.Sequential([\n",
    "  tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(20, input_shape=input_shape)),\n",
    "  tf.keras.layers.Flatten()\n",
    "])\n",
    "\n",
    "# Use `quantize_apply` to actually make the model quantization aware.\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpvX5IqahV1r"
   },
   "source": [
    "## Checkpoint and deserialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T11:06:45.885794Z",
     "iopub.status.busy": "2023-04-06T11:06:45.885263Z",
     "iopub.status.idle": "2023-04-06T11:06:46.081715Z",
     "shell.execute_reply": "2023-04-06T11:06:46.081015Z"
    },
    "id": "6khQg-q7imfH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_4 (QuantizeL  (None, 20)               3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_dense_6 (QuantizeWrap  (None, 20)               425       \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_flatten_6 (QuantizeWr  (None, 20)               1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 420\n",
      "Non-trainable params: 9\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model.\n",
    "base_model = setup_model()\n",
    "base_model.load_weights(pretrained_weights) # optional but recommended for model accuracy\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(base_model)\n",
    "\n",
    "# Save or checkpoint the model.\n",
    "_, keras_model_file = tempfile.mkstemp('.h5')\n",
    "quant_aware_model.save(keras_model_file)\n",
    "\n",
    "# `quantize_scope` is needed for deserializing HDF5 models.\n",
    "with tfmot.quantization.keras.quantize_scope():\n",
    "  loaded_model = tf.keras.models.load_model(keras_model_file)\n",
    "\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeNCMDAbnEKU"
   },
   "source": [
    "## Create and deploy quantized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiYk_KR0rJ2n"
   },
   "source": [
    "In general, reference the documentation for the deployment backend that you\n",
    "will use.\n",
    "\n",
    "This is an example for the TFLite backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T11:06:46.085217Z",
     "iopub.status.busy": "2023-04-06T11:06:46.084737Z",
     "iopub.status.idle": "2023-04-06T11:06:48.134191Z",
     "shell.execute_reply": "2023-04-06T11:06:48.133370Z"
    },
    "id": "fbBiEetda3R8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 228ms/step - loss: 2.1915 - accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_7_layer_call_fn, dense_7_layer_call_and_return_conditional_losses, flatten_7_layer_call_fn, flatten_7_layer_call_and_return_conditional_losses, dense_7_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\YASHMA~1\\AppData\\Local\\Temp\\tmpg3ongz3p\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\YASHMA~1\\AppData\\Local\\Temp\\tmpg3ongz3p\\assets\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    }
   ],
   "source": [
    "base_model = setup_pretrained_model()\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(base_model)\n",
    "\n",
    "# Typically you train the model here.\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "quantized_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5raSy9ghxkv"
   },
   "source": [
    "## Experiment with quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T11:06:48.138059Z",
     "iopub.status.busy": "2023-04-06T11:06:48.137538Z",
     "iopub.status.idle": "2023-04-06T11:06:48.144024Z",
     "shell.execute_reply": "2023-04-06T11:06:48.143311Z"
    },
    "id": "B9SWK5UQT7VQ"
   },
   "outputs": [],
   "source": [
    "LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
    "MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
    "\n",
    "class DefaultDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    # Configure how to quantize weights.\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "      return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    # Configure how to quantize activations.\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "      return [(layer.activation, MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "      # Add this line for each item returned in `get_weights_and_quantizers`\n",
    "      # , in the same order\n",
    "      layer.kernel = quantize_weights[0]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "      # Add this line for each item returned in `get_activations_and_quantizers`\n",
    "      # , in the same order.\n",
    "      layer.activation = quantize_activations[0]\n",
    "\n",
    "    # Configure how to quantize outputs (may be equivalent to activations).\n",
    "    def get_output_quantizers(self, layer):\n",
    "      return []\n",
    "\n",
    "    def get_config(self):\n",
    "      return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vJeoGQG9ZX0"
   },
   "source": [
    "### Quantize custom Keras layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T11:06:48.147153Z",
     "iopub.status.busy": "2023-04-06T11:06:48.146610Z",
     "iopub.status.idle": "2023-04-06T11:06:48.309456Z",
     "shell.execute_reply": "2023-04-06T11:06:48.308737Z"
    },
    "id": "7_rBOJdyWWEs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_6 (QuantizeL  (None, 20)               3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_custom_layer (Quantiz  (None, 20)               425       \n",
      " eWrapperV2)                                                     \n",
      "                                                                 \n",
      " quant_flatten_9 (QuantizeWr  (None, 20)               1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 420\n",
      "Non-trainable params: 9\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
    "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "class CustomLayer(tf.keras.layers.Dense):\n",
    "  pass\n",
    "\n",
    "model = quantize_annotate_model(tf.keras.Sequential([\n",
    "   quantize_annotate_layer(CustomLayer(20, input_shape=(20,)), DefaultDenseQuantizeConfig()),\n",
    "   tf.keras.layers.Flatten()\n",
    "]))\n",
    "\n",
    "# `quantize_apply` requires mentioning `DefaultDenseQuantizeConfig` with `quantize_scope`\n",
    "# as well as the custom Keras layer.\n",
    "with quantize_scope(\n",
    "  {'DefaultDenseQuantizeConfig': DefaultDenseQuantizeConfig,\n",
    "   'CustomLayer': CustomLayer}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnMguvVSnUqD"
   },
   "source": [
    "### Modify quantization parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T11:06:48.312560Z",
     "iopub.status.busy": "2023-04-06T11:06:48.312300Z",
     "iopub.status.idle": "2023-04-06T11:06:48.316781Z",
     "shell.execute_reply": "2023-04-06T11:06:48.316097Z"
    },
    "id": "77jgBjccnTh6"
   },
   "outputs": [],
   "source": [
    "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
    "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "class ModifiedDenseQuantizeConfig(DefaultDenseQuantizeConfig):\n",
    "    # Configure weights to quantize with 4-bit instead of 8-bits.\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "      return [(layer.kernel, LastValueQuantizer(num_bits=4, symmetric=True, narrow_range=False, per_axis=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T11:06:48.319881Z",
     "iopub.status.busy": "2023-04-06T11:06:48.319285Z",
     "iopub.status.idle": "2023-04-06T11:06:48.448396Z",
     "shell.execute_reply": "2023-04-06T11:06:48.447675Z"
    },
    "id": "sq5mfyBF3KxV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_7 (QuantizeL  (None, 20)               3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_dense_9 (QuantizeWrap  (None, 20)               425       \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_flatten_10 (QuantizeW  (None, 20)               1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 420\n",
      "Non-trainable params: 9\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = quantize_annotate_model(tf.keras.Sequential([\n",
    "   # Pass in modified `QuantizeConfig` to modify this Dense layer.\n",
    "   quantize_annotate_layer(tf.keras.layers.Dense(20, input_shape=(20,)), ModifiedDenseQuantizeConfig()),\n",
    "   tf.keras.layers.Flatten()\n",
    "]))\n",
    "\n",
    "# `quantize_apply` requires mentioning `ModifiedDenseQuantizeConfig` with `quantize_scope`:\n",
    "with quantize_scope(\n",
    "  {'ModifiedDenseQuantizeConfig': ModifiedDenseQuantizeConfig}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJMKgzh84CCs"
   },
   "source": [
    "### Modify parts of layer to quantize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T11:06:48.451266Z",
     "iopub.status.busy": "2023-04-06T11:06:48.451015Z",
     "iopub.status.idle": "2023-04-06T11:06:48.455411Z",
     "shell.execute_reply": "2023-04-06T11:06:48.454746Z"
    },
    "id": "6BaaJPBR8djV"
   },
   "outputs": [],
   "source": [
    "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
    "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "class ModifiedDenseQuantizeConfig(DefaultDenseQuantizeConfig):\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "      # Skip quantizing activations.\n",
    "      return []\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "      # Empty since `get_activaations_and_quantizers` returns\n",
    "      # an empty list.\n",
    "      return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T11:06:48.458609Z",
     "iopub.status.busy": "2023-04-06T11:06:48.458090Z",
     "iopub.status.idle": "2023-04-06T11:06:48.600189Z",
     "shell.execute_reply": "2023-04-06T11:06:48.599457Z"
    },
    "id": "Ln9MDIZJ2n3F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_8 (QuantizeL  (None, 20)               3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_dense_10 (QuantizeWra  (None, 20)               423       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_flatten_11 (QuantizeW  (None, 20)               1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 427\n",
      "Trainable params: 420\n",
      "Non-trainable params: 7\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = quantize_annotate_model(tf.keras.Sequential([\n",
    "   # Pass in modified `QuantizeConfig` to modify this Dense layer.\n",
    "   quantize_annotate_layer(tf.keras.layers.Dense(20, input_shape=(20,)), ModifiedDenseQuantizeConfig()),\n",
    "   tf.keras.layers.Flatten()\n",
    "]))\n",
    "\n",
    "# `quantize_apply` requires mentioning `ModifiedDenseQuantizeConfig` with `quantize_scope`:\n",
    "with quantize_scope(\n",
    "  {'ModifiedDenseQuantizeConfig': ModifiedDenseQuantizeConfig}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yD0sIR6tmmRx"
   },
   "source": [
    "### Use custom quantization algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T11:06:48.603568Z",
     "iopub.status.busy": "2023-04-06T11:06:48.602964Z",
     "iopub.status.idle": "2023-04-06T11:06:48.608706Z",
     "shell.execute_reply": "2023-04-06T11:06:48.608053Z"
    },
    "id": "Jt8UioZH49QV"
   },
   "outputs": [],
   "source": [
    "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
    "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "class FixedRangeQuantizer(tfmot.quantization.keras.quantizers.Quantizer):\n",
    "  \"\"\"Quantizer which forces outputs to be between -1 and 1.\"\"\"\n",
    "\n",
    "  def build(self, tensor_shape, name, layer):\n",
    "    # Not needed. No new TensorFlow variables needed.\n",
    "    return {}\n",
    "\n",
    "  def __call__(self, inputs, training, weights, **kwargs):\n",
    "    return tf.keras.backend.clip(inputs, -1.0, 1.0)\n",
    "\n",
    "  def get_config(self):\n",
    "    # Not needed. No __init__ parameters to serialize.\n",
    "    return {}\n",
    "\n",
    "\n",
    "class ModifiedDenseQuantizeConfig(DefaultDenseQuantizeConfig):\n",
    "    # Configure weights to quantize with 4-bit instead of 8-bits.\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "      # Use custom algorithm defined in `FixedRangeQuantizer` instead of default Quantizer.\n",
    "      return [(layer.kernel, FixedRangeQuantizer())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T11:06:48.611608Z",
     "iopub.status.busy": "2023-04-06T11:06:48.611147Z",
     "iopub.status.idle": "2023-04-06T11:06:48.759701Z",
     "shell.execute_reply": "2023-04-06T11:06:48.758988Z"
    },
    "id": "ItC_3mwT2U87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_9 (QuantizeL  (None, 20)               3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_dense_11 (QuantizeWra  (None, 20)               423       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_flatten_12 (QuantizeW  (None, 20)               1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 427\n",
      "Trainable params: 420\n",
      "Non-trainable params: 7\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = quantize_annotate_model(tf.keras.Sequential([\n",
    "   # Pass in modified `QuantizeConfig` to modify this `Dense` layer.\n",
    "   quantize_annotate_layer(tf.keras.layers.Dense(20, input_shape=(20,)), ModifiedDenseQuantizeConfig()),\n",
    "   tf.keras.layers.Flatten()\n",
    "]))\n",
    "\n",
    "# `quantize_apply` requires mentioning `ModifiedDenseQuantizeConfig` with `quantize_scope`:\n",
    "with quantize_scope(\n",
    "  {'ModifiedDenseQuantizeConfig': ModifiedDenseQuantizeConfig}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Tce3stUlHN0L"
   ],
   "name": "training_comprehensive_guide.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "c3a0e1ed7a67280133f8ade5886c8db9f663bbe0c0db84aba701ac80290ec8d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
